{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The imports for this Jupyter notebook and context\n",
    "pandas for DataFrames, data loading and wrangling\n",
    "numpy for math utilities\n",
    "scikit-learn for training a regressor with KFold cross-validation\n",
    "sentence-transformers for the text to vector embeddings to become input features for scikit-learn\n",
    "matplotlib for plotting distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "print(\"All imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for data and procesing\n",
    "\n",
    "pandas (load/clean) → sentence-transformers (embed text) → numpy (store as arrays) → scikit-learn (train/eval models) → matplotlib (visualize results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files in dummy_data\n",
      "These are the files: ['dummy_data/testdataset.csv']\n",
      "==================================================\n",
      "Successfully loaded testdataset.csv with utf-8 encoding\n",
      "Cleaning testdataset.csv:\n",
      "  Original shape: (7, 4)\n",
      "  Cleaned shape: (2, 4)\n",
      "  Columns: ['issuekey', 'title', 'description', 'storypoint']\n",
      "  Data types: {'issuekey': dtype('O'), 'title': dtype('O'), 'description': dtype('O'), 'storypoint': dtype('int64')}\n",
      "\n",
      "Sample data from testdataset.csv:\n",
      "  issuekey                                              title  \\\n",
      "5   TESB-6  S9 Investigate MDM Call ESB Provider from MDM ...   \n",
      "6  TESB-33  Common - Setup ESB Runtime Code Repositories (...   \n",
      "\n",
      "                                         description  storypoint  \n",
      "5                                            NULL123           5  \n",
      "6  Code Repository: It is expected that we setup ...           3  \n",
      "==================================================\n",
      "\n",
      "Successfully loaded 1 datasets:\n",
      "  testdataset: 2 rows, 4 columns\n",
      "    No missing values (all NULL entries completely removed)\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset directory\n",
    "# dataset_dir = \"data/datasets/storypoint/IEEE TSE2018/dataset\"\n",
    "dataset_dir = \"dummy_data\" # This is a test csv to ensure cleaning of NULL values.\n",
    "# Function to clean individual dataframes\n",
    "def clean_dataframe(df, filename):\n",
    "    \"\"\"Clean a single dataframe with common preprocessing steps.\"\"\"\n",
    "    print(f\"Cleaning {filename}:\")\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    \n",
    "    # Replace various NULL representations with actual NaN\n",
    "    df = df.replace(['NULL', 'null', 'Null', ''], pd.NA)\n",
    "    \n",
    "    # Remove rows where ANY column has NaN/NULL values\n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    # Remove completely empty rows and columns\n",
    "    df = df.dropna(how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # Strip whitespace from string columns\n",
    "    string_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        # Remove rows with 'nan' strings that might have been created\n",
    "        df = df[df[col] != 'nan']\n",
    "        df = df[df[col] != 'None']\n",
    "    \n",
    "    # Convert numeric columns (story points, etc.)\n",
    "    numeric_keywords = ['storypoint']\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in numeric_keywords):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Remove rows where numeric conversion failed\n",
    "            df = df.dropna(subset=[col])\n",
    "    \n",
    "    print(f\"  Cleaned shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Data types: {df.dtypes.to_dict()}\")\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Import all CSV files\n",
    "csv_files = glob.glob(os.path.join(dataset_dir, \"*.csv\"))\n",
    "dataframes = {}\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files in {dataset_dir}\")\n",
    "print(f\"These are the files: {csv_files}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    filename = Path(csv_file).name\n",
    "    try:\n",
    "        # Try different encodings\n",
    "        df = None\n",
    "        for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "            try:\n",
    "                # Read CSV keeping NULL as strings initially so we can handle them properly\n",
    "                df = pd.read_csv(csv_file, encoding=encoding, keep_default_na=False)\n",
    "                print(f\"Successfully loaded {filename} with {encoding} encoding\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to load {filename} with {encoding} encoding\")\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            # If all encodings fail, use utf-8 with error handling\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8', errors='ignore', keep_default_na=False)\n",
    "            print(f\"Loaded {filename} with utf-8 encoding and error handling\")\n",
    "        \n",
    "        # Clean the dataframe\n",
    "        df_cleaned = clean_dataframe(df, filename)\n",
    "        \n",
    "        if len(df_cleaned) > 0:  # Only store if we have data left after cleaning\n",
    "            # Store with filename as key (without .csv extension)\n",
    "            key = filename.replace('.csv', '')\n",
    "            dataframes[key] = df_cleaned\n",
    "            \n",
    "            # Display first few rows\n",
    "            print(f\"Sample data from {filename}:\")\n",
    "            print(df_cleaned.head())\n",
    "        else:\n",
    "            print(f\"No data remaining after cleaning {filename}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSuccessfully loaded {len(dataframes)} datasets:\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"  {name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"    No missing values (all NULL entries completely removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape for testdataset: (2, 384)\n",
      "testdataset: 2 rows, embedding dimension: (384,)\n",
      "Type of stored embedding: <class 'numpy.ndarray'>\n",
      "dataframe   issuekey                                              title  \\\n",
      "5   TESB-6  S9 Investigate MDM Call ESB Provider from MDM ...   \n",
      "6  TESB-33  Common - Setup ESB Runtime Code Repositories (...   \n",
      "\n",
      "                                         description  storypoint  \\\n",
      "5                                            NULL123           5   \n",
      "6  Code Repository: It is expected that we setup ...           3   \n",
      "\n",
      "                                          embeddings  \n",
      "5  [-0.062368233, 0.059396103, -0.09936537, -0.06...  \n",
      "6  [-0.013562303, -0.13194102, 0.020181717, -0.06...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Calculate the embeddings and add as a new column to the dataframe for the story\n",
    "    sentences = df['description'].tolist()\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    print(f\"Embeddings Shape for {name}: {embeddings.shape}\")\n",
    "    \n",
    "    # Store as numpy arrays for scikit-learn\n",
    "    df['embeddings'] = [np.array(emb) for emb in embeddings]\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {df.shape[0]} rows, embedding dimension: {df['embeddings'].iloc[0].shape}\")\n",
    "    # print(f\"Type of stored embedding: {type(df['embeddings'].iloc[0])}\")\n",
    "    print(f\"Dataframe: {df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StoryPointer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
